import streamlit as st
import torch
from torchvision import transforms
from PIL import Image
import openai
import requests
from io import BytesIO
import base64
import numpy as np
from utils import (
    load_baseline_model,
    generate_baseline_caption,
    enhance_with_openai,
    load_image,
    preprocess_image
)

# --- Configuration ---
st.set_page_config(page_title="Seeing & Speaking", layout="wide")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- Model Loading ---
@st.cache_resource
def get_models():
    with st.spinner("üîÑ Loading models... hang tight!"):
        return load_baseline_model()

encoder, decoder, vocab = get_models()

# --- Helper Functions ---
def encode_image_to_base64(image):
    """Convert PIL image to base64 for OpenAI API"""
    buffered = BytesIO()
    image.save(buffered, format="JPEG")
    return base64.b64encode(buffered.getvalue()).decode('utf-8')

def get_gpt4_vision_caption(base64_image):
    try:
        client = openai.OpenAI(api_key=st.secrets["openai_key"])
        response = client.chat.completions.create(
            model="gpt-4-vision-preview",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "Describe this image accurately and concisely."},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}"
                            }
                        }
                    ]
                }
            ],
            max_tokens=300
        )
        return response.choices[0].message.content
    except Exception as e:
        st.error(f"GPT-4 Vision Error: {str(e)}")
        return None


# --- UI Layout ---
st.title("Vision to Text: Baseline üÜö OpenAI")
st.caption("Compare: Baseline CNN-RNN model | GPT-3.5 Enhanced | GPT-4 Vision")

# Sidebar
with st.sidebar:
    st.header("Settings")
    openai_enabled = st.toggle("Enable OpenAI", True)
    st.divider()
    st.markdown("""
    **Model Details**  
    - Baseline: ResNet50 + Attention LSTM  
    - OpenAI: GPT-3.5 Turbo  
    [View Baseline Model](https://huggingface.co/weakyy/image-captioning-baseline-model)
    """)
    st.write("Model Status:", 
        f"Encoder: {'Loaded' if encoder else 'Failed'}",
        f"Decoder: {'Loaded' if decoder else 'Failed'}",
        f"Vocab Size: {len(vocab['word2idx']) if vocab else 0}"
    )

# Image Upload or Example
uploaded_file = st.file_uploader("Upload an image", type=["jpg", "png", "jpeg"])
example_images = {
    "Beach": "https://images.unsplash.com/photo-1507525428034-b723cf961d3e?w=600",
    "Dog": "https://images.unsplash.com/photo-1561037404-61cd46aa615b?w=600",
    "Food": "https://images.unsplash.com/photo-1565958011703-72f8583c2708?w=600"
}

if not uploaded_file:
    selected = st.selectbox("Or try an example:", list(example_images.keys()))
    image = load_image(example_images[selected])
else:
    image = load_image(uploaded_file)

baseline_result = None
if image:
    st.image(image, caption="Input Image", use_container_width=True)
    image_tensor = preprocess_image(image, device=device)
    base64_image = encode_image_to_base64(image)

    col1, col2, col3 = st.columns(3)

    # 1. Baseline Model
    with col1:
        st.subheader("üß† Baseline CNN+RNN Model")
        with st.spinner("Generating baseline CNN-RNN caption..."):
            try:
                baseline_result = generate_baseline_caption(
                    image_tensor=image_tensor,
                    encoder=encoder,
                    decoder=decoder,
                    vocab=vocab,
                    beam_size=3
                )
                st.success(baseline_result["caption"])
                st.caption(f"Confidence: {baseline_result['confidence']:.0%}")
            except Exception as e:
                st.error(f"Error generating baseline caption: {str(e)}")
                baseline_result = None

        st.write("Rate this caption:")
        if st.button("üëç", key="like_baseline"):
            st.toast("Thanks for your feedback!")
        st.button("üëé", key="dislike_baseline")

    # 2. GPT-3.5 Enhanced Caption
    with col2:
        st.subheader("üîç GPT-3.5 Enhanced")
        if openai_enabled and 'openai_key' in st.secrets and baseline_result:
            with st.spinner("Enhancing caption with GPT-3.5..."):
                enhanced = enhance_with_openai(baseline_result["caption"])
                if enhanced:
                    st.success(enhanced)
                else:
                    st.error("Enhancement failed")
        else:
            st.warning("Baseline caption not available or OpenAI not enabled.")

        st.write("Rate this enhancement:")
        if st.button("üëç", key="like_openai_enhancement"):
            st.toast("Thanks for your feedback!")
        st.button("üëé", key="dislike_openai_enhancement")

    # 3. GPT-4 Vision
    with col3:
        st.subheader("‚ú® GPT-4 Vision")
        if 'openai_key' in st.secrets:
            with st.spinner("Analyzing image..."):
                vision_caption = get_gpt4_vision_caption(base64_image)
                if vision_caption:
                    st.success(vision_caption)
                else:
                    st.error("Vision analysis failed")
        else:
            st.warning("Add OpenAI key to enable")

# Footer
st.divider()
st.caption("""
‚ö° **Tip**: Baseline CNN+RNN uses the trained CNN-RNN, GPT-3.5 refines that output, 
while GPT-4 Vision generates captions directly from pixels
""")
st.markdown("""
[GitHub Repo](https://github.com/your-repo) | 
[Model Card](https://huggingface.co/weakyy/image-captioning-baseline-model)
""")
